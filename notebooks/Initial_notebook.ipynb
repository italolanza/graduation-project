{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path related configuration\n",
    "DATASET_BASE_DIR = Path(\"/home/italolanza/workspace/TG/dataset\")\n",
    "\n",
    "NORMAL_FILES = glob.glob(str(DATASET_BASE_DIR) + '/normal/*.csv')\n",
    "\n",
    "HOR_MISALIGNMENT_LOW_FILES = glob.glob(str(DATASET_BASE_DIR) + '/horizontal/0.5mm/*.csv')\n",
    "\n",
    "HOR_MISALIGNMENT_MEDIUM_FILES = glob.glob(str(DATASET_BASE_DIR) + '/horizontal/1.0mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/horizontal/1.5mm/*.csv')\n",
    "\n",
    "HOR_MISALIGNMENT_HIGH_FILES = glob.glob(str(DATASET_BASE_DIR) + '/horizontal/2.0mm/*.csv')\n",
    "\n",
    "VER_MISALIGNMENT_LOW_FILES = glob.glob(str(DATASET_BASE_DIR) + '/vertical/0.51mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/vertical/0.63mm/*.csv')\n",
    "\n",
    "VER_MISALIGNMENT_MEDIUM_FILES = glob.glob(str(DATASET_BASE_DIR) + '/vertical/1.27mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/vertical/1.40mm/*.csv')\n",
    "\n",
    "VER_MISALIGNMENT_HIGH_FILES = glob.glob(str(DATASET_BASE_DIR) + '/vertical/1.78mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/vertical/1.90mm/*.csv')\n",
    "\n",
    "IMBALANCE_LOW_FILES = glob.glob(str(DATASET_BASE_DIR) + '/imbalance/6g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/10g/*.csv')\n",
    "\n",
    "IMBALANCE_MEDIUM_FILES = glob.glob(str(DATASET_BASE_DIR) + '/imbalance/15g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/20g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/25g/*.csv')\n",
    "\n",
    "IMBALANCE_HIGH_FILES = glob.glob(str(DATASET_BASE_DIR) + '/imbalance/30g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/35g/*.csv')\n",
    "\n",
    "OUTPUT_DATA_DIR = DATASET_BASE_DIR.joinpath(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal data\n",
    "def process_normal_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    if (Path.exists(OUTPUT_DATA_DIR.joinpath(\"normal_data.csv\"))):\n",
    "        return\n",
    " \n",
    "    \n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    NORMAL_DATA_OUTPUT = [0, 0.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    for file_name in NORMAL_FILES:\n",
    "\n",
    "        data_list = list()\n",
    "\n",
    "        with open(file_name, 'r') as data_file:\n",
    "            data_iter = csv.reader(data_file, delimiter=\",\")            \n",
    "            for data in data_iter:\n",
    "                data.extend(NORMAL_DATA_OUTPUT)\n",
    "                data_list.append(data)\n",
    "\n",
    "        if (suffle_data):\n",
    "            data_list.sort()\n",
    "        \n",
    "        with open(OUTPUT_DATA_DIR.joinpath(\"normal_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "             open(OUTPUT_DATA_DIR.joinpath(\"normal_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "            data_size = len(data_list)\n",
    "            test_index = floor(data_size * test_size)\n",
    "            total_lines += data_size \n",
    "           \n",
    "            test_writer = csv.writer(test_file)\n",
    "            training_writer = csv.writer(training_file)\n",
    "\n",
    "            test_writer.writerows(data_list[:test_index])\n",
    "            training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance data\n",
    "def process_imbalance_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    #IMBALANCE_OUTPUT_FILES = [\"imbalance_low_data.csv\", \"imbalance_medium_data.csv\", \"imbalance_high_data.csv\"]\n",
    "\n",
    "\n",
    "    # if (Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_data.csv\"))):\n",
    "    #     return\n",
    " \n",
    "\n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    LOW_IMBALANCE_OUTPUT = [1, 1.0]\n",
    "    MEDIUM_IMBALANCE_OUTPUT = [1, 2.0]\n",
    "    HIGH_IMBALANCE_OUTPUT = [1, 3.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Low criticality (6g, 10g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_LOW_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")\n",
    "                for data in data_iter:\n",
    "                    data.extend(LOW_IMBALANCE_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])                \n",
    "    \n",
    "\n",
    "    print(\"Low data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Medium criticality (15g, 20g, 25g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_MEDIUM_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(MEDIUM_IMBALANCE_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "\n",
    "    \n",
    "    print(\"Medium data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # High criticality (30g, 35g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_HIGH_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(HIGH_IMBALANCE_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "\n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"High data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal misalignment\n",
    "def process_hor_misalignment_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    # HOR_MISLAGNMENT_OUTPUT_FILES=[\"hor_misalignment_low_data.csv\", \"hor_misalignment_medium_data.csv\", \"hor_misalignment_high_data.csv\"]\n",
    "\n",
    "\n",
    "    # if (Path.exists( OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_data.csv\")) ):\n",
    "    #     return\n",
    "\n",
    " \n",
    "\n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    LOW_HOR_MISALIGNMENT_OUTPUT = [2, 1.0]\n",
    "    MEDIUM_HOR_MISALIGNMENT_OUTPUT = [2, 2.0]\n",
    "    HIGH_HOR_MISALIGNMENT_OUTPUT = [2, 3.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Low criticality (6g, 10g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_LOW_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")\n",
    "                for data in data_iter:\n",
    "                    data.extend(LOW_HOR_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Low data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # Medium criticality (15g, 20g, 25g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_MEDIUM_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(MEDIUM_HOR_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Medium data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # High criticality (30g, 35g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_HIGH_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(HIGH_HOR_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)    \n",
    "\n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "            \n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"High data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical misalignment\n",
    "def process_ver_misalignment_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    # VER_MISLAGNMENT_OUTPUT_FILES=[\"ver_misalignment_low_data.csv\", \"ver_misalignment_medium_data.csv\", \"ver_misalignment_high_data.csv\"]\n",
    "\n",
    "\n",
    "    # if (Path.exists( OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_data.csv\")) ):\n",
    "    #     return\n",
    "\n",
    " \n",
    "\n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    LOW_VER_MISALIGNMENT_OUTPUT = [3, 1.0]\n",
    "    MEDIUM_VER_MISALIGNMENT_OUTPUT = [3, 2.0]\n",
    "    HIGH_VER_MISALIGNMENT_OUTPUT = [3, 3.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Low criticality (6g, 10g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_LOW_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")\n",
    "                for data in data_iter:\n",
    "                    data.extend(LOW_VER_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                data_list.sort()               \n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Low data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # Medium criticality (15g, 20g, 25g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_MEDIUM_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(MEDIUM_VER_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "\n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Medium data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # High criticality (30g, 35g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_HIGH_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(HIGH_VER_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)    \n",
    "\n",
    "            if (suffle_data):\n",
    "                data_list.sort()\n",
    "            \n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"High data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process normal data\n",
    "#process_normal_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process imbalance data\n",
    "#process_imbalance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process horizontal misalignment data\n",
    "#process_hor_misalignment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process vertical  misalignment data\n",
    "#process_ver_misalignment_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to get the normal data\n",
    "def get_normal_data(chunk_size):\n",
    "    if (Path.exists(OUTPUT_DATA_DIR.joinpath(\"normal_data.csv\"))):\n",
    "        df = pd.read_csv(OUTPUT_DATA_DIR.joinpath(\"normal_data.csv\"),chunksize=chunk_size )\n",
    "        return df\n",
    "\n",
    "\n",
    "# Func to get the imbalance data\n",
    "def get_imbalance_data(chunk_size):\n",
    "\n",
    "    IMBALANCE_OUTPUT_FILES = [\"imbalance_low_data.csv\", \"imbalance_medium_data.csv\", \"imbalance_high_data.csv\"]\n",
    "    ImbalanceData = namedtuple(\"ImbalanceData\", \"low_imb_data med_imb_data high_imb_data\")\n",
    "    \n",
    "    data = list()\n",
    "\n",
    "\n",
    "    if Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data.csv\")) \\\n",
    "        and Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data.csv\")) \\\n",
    "        and Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data.csv\")):\n",
    "\n",
    "        for data_type in IMBALANCE_OUTPUT_FILES:\n",
    "            data.append(pd.read_csv(OUTPUT_DATA_DIR.joinpath(data_type),chunksize=chunk_size ))\n",
    "\n",
    "    return ImbalanceData(data[0],data[1],data[2])\n",
    "\n",
    "\n",
    "# Func to get the horizontal misalignment data\n",
    "def get_horizontal_misalignment_data(chunk_size):\n",
    "\n",
    "    HOR_MISALIGNMENT_OUTPUT_FILES = [\"hor_misalignment_low_data.csv\", \"hor_misalignment_medium_data.csv\", \"hor_misalignment_high_data.csv\"]\n",
    "    HorMisalignmentData = namedtuple(\"HorMisalignmentData\", [\"low_hor_mis_data\", \"med_hor_mis_data\", \"high_hor_mis_data\"])\n",
    "    \n",
    "    data = list()\n",
    "\n",
    "\n",
    "    if Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data.csv\")) \\\n",
    "        and Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data.csv\")) \\\n",
    "        and Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data.csv\")):\n",
    "\n",
    "        for data_type in HOR_MISALIGNMENT_OUTPUT_FILES:\n",
    "            data.append(pd.read_csv(OUTPUT_DATA_DIR.joinpath(data_type),chunksize=chunk_size ))\n",
    "\n",
    "    return HorMisalignmentData(data[0],data[1],data[2])\n",
    "\n",
    "\n",
    "# Func to get the vertical misalignment data\n",
    "def get_vertical_misalignment_data(chunk_size):\n",
    "\n",
    "    VER_MISALIGNMENT_OUTPUT_FILES = [\"ver_misalignment_low_data.csv\", \"ver_misalignment_medium_data.csv\", \"ver_misalignment_high_data.csv\"]\n",
    "    VerMisalignmentData = namedtuple(\"VerMisalignmentData\", [\"low_ver_mis_data\", \"med_ver_mis_data\", \"high_ver_mis_data\"])\n",
    "    \n",
    "    data = list()\n",
    "\n",
    "\n",
    "    if Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data.csv\")) \\\n",
    "        and Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data.csv\")) \\\n",
    "        and Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data.csv\")):\n",
    "\n",
    "        for data_type in VER_MISALIGNMENT_OUTPUT_FILES:\n",
    "            data.append(pd.read_csv(OUTPUT_DATA_DIR.joinpath(data_type),chunksize=chunk_size ))\n",
    "\n",
    "    return VerMisalignmentData(data[0],data[1],data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_DATASET_SIZE_TRAINING = 8575000 # The number of lines of the training set of the Normal data (the smaller)\n",
    "    MAX_DATASET_SIZE_VALIDATION = 3675000 # The number of lines of the validation set of the Normal data (the smaller)\n",
    "\n",
    "    def __init__(self, batch_size=8575, is_validation=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.is_validation = is_validation\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns the number of batches in the sequence\n",
    "        if not self.is_validation:\n",
    "            return int(floor(DataGenerator.MAX_DATASET_SIZE_TRAINING / self.batch_size))\n",
    "        else:\n",
    "            return int(floor(DataGenerator.MAX_DATASET_SIZE_VALIDATION / self.batch_size))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns the next batch of values based on the index\n",
    "        name_modifier = \"\"\n",
    "        data = pd.DataFrame()\n",
    "        \n",
    "\n",
    "        if self.is_validation:\n",
    "            name_modifier = \"_validacao\"\n",
    "\n",
    "        else:\n",
    "            name_modifier = \"_treinamento\"\n",
    "\n",
    "           \n",
    "        normal_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"normal_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        imbalance_low_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"imbalance_low_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size))\n",
    "        imbalance_medium_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"imbalance_medium_data{name_modifier}.csv\"), chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        imbalance_high_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"imbalance_high_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        hor_misalignment_low_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"hor_misalignment_low_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        hor_misalignment_medium_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"hor_misalignment_medium_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        hor_misalignment_high_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"hor_misalignment_high_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        ver_misalignment_low_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"ver_misalignment_low_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        ver_misalignment_medium_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"ver_misalignment_medium_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        ver_misalignment_high_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"ver_misalignment_high_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "\n",
    "\n",
    "        data = data.append(normal_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(imbalance_low_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(imbalance_medium_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(imbalance_high_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(hor_misalignment_low_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(hor_misalignment_medium_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(hor_misalignment_high_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(ver_misalignment_low_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(ver_misalignment_medium_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(ver_misalignment_high_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "\n",
    "\n",
    "        data_x = data.iloc[:,0:8].to_numpy()\n",
    "        data_y = data.iloc[:,8:10].to_numpy()\n",
    "\n",
    "        return (data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
