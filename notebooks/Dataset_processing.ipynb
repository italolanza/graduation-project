{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis\n",
    "import csv\n",
    "from math import floor\n",
    "import npeet.entropy_estimators as ee\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path related configuration\n",
    "DATASET_BASE_DIR = Path(\"/home/italolanza/workspace/TG/dataset_raw/\")\n",
    "\n",
    "NORMAL_BASE_DIR = Path(str(DATASET_BASE_DIR) + '/normal/')\n",
    "\n",
    "NORMAL_FILES = glob.glob(str(DATASET_BASE_DIR) + '/normal/*.csv')\n",
    "\n",
    "# imbalance data\n",
    "IMBALANCE_BASE_DIR = Path(str(DATASET_BASE_DIR) + '/imbalance/')\n",
    "\n",
    "IMBALANCE_LOW_FILES = glob.glob(str(DATASET_BASE_DIR) + '/imbalance/6g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/10g/*.csv')\n",
    "\n",
    "IMBALANCE_MEDIUM_FILES = glob.glob(str(DATASET_BASE_DIR) + '/imbalance/15g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/20g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/25g/*.csv')\n",
    "\n",
    "IMBALANCE_HIGH_FILES = glob.glob(str(DATASET_BASE_DIR) + '/imbalance/30g/*.csv') \\\n",
    "                        + glob.glob(str(DATASET_BASE_DIR) + '/imbalance/35g/*.csv')\n",
    "\n",
    "# horizontal misalignment data\n",
    "HOR_MISALIGNMENT_BASE_DIR = Path(str(DATASET_BASE_DIR) + '/horizontal-misalignment/')\n",
    "\n",
    "HOR_MISALIGNMENT_LOW_FILES = glob.glob(str(DATASET_BASE_DIR) + '/horizontal-misalignment/0.5mm/*.csv')\n",
    "\n",
    "HOR_MISALIGNMENT_MEDIUM_FILES = glob.glob(str(DATASET_BASE_DIR) + '/horizontal-misalignment/1.0mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/horizontal-misalignment/1.5mm/*.csv')\n",
    "\n",
    "HOR_MISALIGNMENT_HIGH_FILES = glob.glob(str(DATASET_BASE_DIR) + '/horizontal-misalignment/2.0mm/*.csv')\n",
    "\n",
    "# vertigal misalignment data\n",
    "VER_MISALIGNMENT_BASE_DIR = Path(str(DATASET_BASE_DIR) + '/vertical-misalignment/')\n",
    "\n",
    "VER_MISALIGNMENT_LOW_FILES = glob.glob(str(DATASET_BASE_DIR) + '/vertical-misalignment/0.51mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/vertical-misalignment/0.63mm/*.csv')\n",
    "\n",
    "VER_MISALIGNMENT_MEDIUM_FILES = glob.glob(str(DATASET_BASE_DIR) + '/vertical-misalignment/1.27mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/vertical-misalignment/1.40mm/*.csv')\n",
    "\n",
    "VER_MISALIGNMENT_HIGH_FILES = glob.glob(str(DATASET_BASE_DIR) + '/vertical-misalignment/1.78mm/*.csv') \\\n",
    "                                + glob.glob(str(DATASET_BASE_DIR) + '/vertical-misalignment/1.90mm/*.csv')\n",
    "#overhang data\n",
    "OVERHANG_MISALIGNMENT_BASE_DIR = Path(str(DATASET_BASE_DIR) + '/overhang/')\n",
    "#underhang data\n",
    "UNDERHANG_MISALIGNMENT_BASE_DIR = Path(str(DATASET_BASE_DIR) + '/underhang/')\n",
    "\n",
    "\n",
    "OUTPUT_DATA_DIR = Path(\"/home/italolanza/workspace/TG/dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 50000 # Sample rate\n",
    "T = 1/sampling_rate # Sampling period\n",
    "time = np.arange(0, 5, T) # Create a time array from 0 to 5 seconds\n",
    "sensors = ['Tacom', 'Aceler_Underhang_X','Aceler_Underhang_Y', 'Aceler_Underhang_Z', 'Aceler_Overhang_X', 'Aceler_Overhang_Y', 'Aceler_Overhang_Z', 'Audio']\n",
    "\n",
    "features = []\n",
    "for name in sensors:\n",
    "    for i in range(1,4):\n",
    "        column_name = name+'_'+str(i)+'f0'\n",
    "        features.append(column_name)\n",
    "    column_name = name+'_'+'kurtosis'\n",
    "    features.append(column_name)\n",
    "    column_name = name+'_'+'entropy'\n",
    "    features.append(column_name)\n",
    "    \n",
    "features.append('Class')\n",
    "print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(root_dir=\"\", defect_class=\"\", single_file=True, suffle_data=True, test_size=0.3):\n",
    "    \"\"\"\n",
    "    It creates a validation and training dataset from the file list\n",
    "    \"\"\"\n",
    "    \n",
    "    defect_class = \"\"\n",
    "    sensor_features = [] # List to store features\n",
    "\n",
    "    for root, dirs, files_list in os.walk(root_dir):\n",
    "\n",
    "        print(\"Checking directory:\" + root)\n",
    "        splitted_root = root.split(\"/\")\n",
    "        \n",
    "        if(len(splitted_root) > 6):\n",
    "            defect_class = splitted_root[6]\n",
    "        \n",
    "        # Define class for classification\n",
    "        defect_value = -1 # unitialized\n",
    "        if(defect_class == 'normal'):\n",
    "            defect_value = 0\n",
    "        elif(defect_class == 'imbalance'):\n",
    "            defect_value = 1\n",
    "        elif(defect_class == 'horizontal-misalignment'):\n",
    "            defect_value = 2\n",
    "        elif(defect_class == 'vertical-misalignment'):\n",
    "            defect_value = 3\n",
    "        elif(defect_class == 'overhang'):\n",
    "            defect_value = 4\n",
    "        elif(defect_class == 'underhang'):\n",
    "            defect_value = 5\n",
    "\n",
    "        for file in files_list:\n",
    "\n",
    "            file_path = root + '/' + file\n",
    "            \n",
    "            # Read data\n",
    "            print(f'Opening file: {file_path}')\n",
    "            file_data = pd.read_csv(file_path, names=sensors)\n",
    "\n",
    "            # Compute Fourer Transform\n",
    "            fourier_transform_normal = np.fft.rfft(file_data['Tacom'].values) # Calculando a tranformada de fourier do sinal(espectro)\n",
    "            abs_fourier_transform_normal = np.abs(fourier_transform_normal) # Calcula o valor absoluto do números (Amplitude do espectro)\n",
    "            power_spectrum_normal = np.square(abs_fourier_transform_normal) # Tira a raiz do valor absoluto\n",
    "            frequency_normal = np.linspace(0, sampling_rate/2, len(power_spectrum_normal)) # Contruindo vetor das frequencias calculadas \n",
    "\n",
    "            # Find peaks\n",
    "            Temp = file_data['Tacom'] > np.amax(file_data['Tacom'].values)/2 # Temporário\n",
    "            # Vetor de booleanos\n",
    "            peaks, _ = find_peaks(Temp) # Encontra os picos\n",
    "            \n",
    "            # print(peaks)\n",
    "\n",
    "            Tempo_entre_picos = np.mean(np.diff(peaks))*T # Time between peaks\n",
    "            f = 1/Tempo_entre_picos # Frequency\n",
    "\n",
    "            # Harmonic frequencies\n",
    "            ind_freq_rot1 = np.argwhere(np.abs(frequency_normal-f)<0.1) # Econtrando o valor da frequencia obtida f dentro do vetor \n",
    "            ind_freq_rot2 = np.argwhere(np.abs(frequency_normal-2*f)<0.1) # Econtrando o valor da frequencia obtida f dentro do vetor\n",
    "            ind_freq_rot3 = np.argwhere(np.abs(frequency_normal-3*f)<0.1) # Econtrando o valor da frequencia obtida f dentro do vetor\n",
    "\n",
    "            feature_record = []\n",
    "            for column in file_data:\n",
    "\n",
    "                # print(column)\n",
    "                \n",
    "                # Compute Fourier transform at each sensor\n",
    "                fourier_transform_normal = np.fft.rfft(file_data[column].values) # Calculando a tranformada de fourier do sinal(espectro)\n",
    "                abs_fourier_transform_normal = np.abs(fourier_transform_normal) # Calcula o valor absoluto do números (Amplitude do espectro)\n",
    "                power_spectrum_normal = np.square(abs_fourier_transform_normal) # Tira a raiz do valor absoluto\n",
    "                frequency_normal = np.linspace(0, sampling_rate/2, len(power_spectrum_normal)) # Contruindo vetor das frequencias calculadas \n",
    "\n",
    "                # Find amplitudes at harmonic frequencies\n",
    "                abs_fourier_transform_f0 = abs_fourier_transform_normal[ind_freq_rot1] # Amplitude do espectro f0\n",
    "                abs_fourier_transform_2f0 = abs_fourier_transform_normal[ind_freq_rot2] # Amplitude do espectro 2xf0\n",
    "                abs_fourier_transform_3f0 = abs_fourier_transform_normal[ind_freq_rot3] # Amplitude do espectro 3xf0\n",
    "                \n",
    "                #print(file_data)\n",
    "                feature_record.append(abs_fourier_transform_f0[0,0])\n",
    "                feature_record.append(abs_fourier_transform_2f0[0,0])\n",
    "                feature_record.append(abs_fourier_transform_3f0[0,0])\n",
    "                \n",
    "                # Find Kurtosi and Entropy\n",
    "                curtose_normal = kurtosis(file_data[column]) #Calcula a curtose do sinal \n",
    "                list_normal = list(map(lambda x: [x], file_data[column].tolist())) #Adiciona os sinais a uma lista para o cáclulo da entropia\n",
    "                entropy_normal = ee.entropy(list_normal) #Calcula a entropia\n",
    "                \n",
    "                feature_record.append(curtose_normal) #Adiciona a curtose\n",
    "                feature_record.append(entropy_normal) #Adiciona a entropia\n",
    "\n",
    "            # add defect class value\n",
    "            feature_record.append(defect_value)\n",
    "\n",
    "            \n",
    "            # append list with features\n",
    "            sensor_features.append(feature_record)\n",
    "\n",
    "    \n",
    "    #Already processed all files, creates a DataFrame\n",
    "    dataset = pd.DataFrame(sensor_features, columns=features)\n",
    "    if single_file:\n",
    "        #Write dataset to csv\n",
    "        dataset.to_csv(f'{str(OUTPUT_DATA_DIR)}/dataset_completo.csv', header=False, mode='a', index=False)\n",
    "    else:\n",
    "        #Write dataset to csv\n",
    "        dataset.to_csv(f'{str(OUTPUT_DATA_DIR)}/{defect_class}_data.csv', header=False, mode='a', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal data\n",
    "def process_normal_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    if (Path.exists(OUTPUT_DATA_DIR.joinpath(\"normal_data.csv\"))):\n",
    "        return\n",
    " \n",
    "    \n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    NORMAL_DATA_OUTPUT = [0, 0.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    for file_name in NORMAL_FILES:\n",
    "\n",
    "        data_list = list()\n",
    "\n",
    "        with open(file_name, 'r') as data_file:\n",
    "            data_iter = csv.reader(data_file, delimiter=\",\")            \n",
    "            for data in data_iter:\n",
    "                data.extend(NORMAL_DATA_OUTPUT)\n",
    "                data_list.append(data)\n",
    "\n",
    "        if (suffle_data):\n",
    "            random.suffle(data_list)\n",
    "        \n",
    "        with open(OUTPUT_DATA_DIR.joinpath(\"normal_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "             open(OUTPUT_DATA_DIR.joinpath(\"normal_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "            data_size = len(data_list)\n",
    "            test_index = floor(data_size * test_size)\n",
    "            total_lines += data_size \n",
    "           \n",
    "            test_writer = csv.writer(test_file)\n",
    "            training_writer = csv.writer(training_file)\n",
    "\n",
    "            test_writer.writerows(data_list[:test_index])\n",
    "            training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance data\n",
    "def process_imbalance_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    #IMBALANCE_OUTPUT_FILES = [\"imbalance_low_data.csv\", \"imbalance_medium_data.csv\", \"imbalance_high_data.csv\"]\n",
    "\n",
    "\n",
    "    # if (Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_data.csv\"))):\n",
    "    #     return\n",
    " \n",
    "\n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    LOW_IMBALANCE_OUTPUT = [1, 1.0]\n",
    "    MEDIUM_IMBALANCE_OUTPUT = [1, 2.0]\n",
    "    HIGH_IMBALANCE_OUTPUT = [1, 3.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Low criticality (6g, 10g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_LOW_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")\n",
    "                for data in data_iter:\n",
    "                    data.extend(LOW_IMBALANCE_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"imbalance_low_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])                \n",
    "    \n",
    "\n",
    "    print(\"Low data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Medium criticality (15g, 20g, 25g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_MEDIUM_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(MEDIUM_IMBALANCE_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"imbalance_medium_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "\n",
    "    \n",
    "    print(\"Medium data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # High criticality (30g, 35g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_HIGH_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(HIGH_IMBALANCE_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "\n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"imbalance_high_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"High data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal misalignment\n",
    "def process_hor_misalignment_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    # HOR_MISLAGNMENT_OUTPUT_FILES=[\"hor_misalignment_low_data.csv\", \"hor_misalignment_medium_data.csv\", \"hor_misalignment_high_data.csv\"]\n",
    "\n",
    "\n",
    "    # if (Path.exists( OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_data.csv\")) ):\n",
    "    #     return\n",
    "\n",
    " \n",
    "\n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    LOW_HOR_MISALIGNMENT_OUTPUT = [2, 1.0]\n",
    "    MEDIUM_HOR_MISALIGNMENT_OUTPUT = [2, 2.0]\n",
    "    HIGH_HOR_MISALIGNMENT_OUTPUT = [2, 3.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Low criticality (6g, 10g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_LOW_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")\n",
    "                for data in data_iter:\n",
    "                    data.extend(LOW_HOR_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_low_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Low data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # Medium criticality (15g, 20g, 25g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_MEDIUM_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(MEDIUM_HOR_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_medium_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Medium data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # High criticality (30g, 35g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_HIGH_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(HIGH_HOR_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)    \n",
    "\n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "            \n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"hor_misalignment_high_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"High data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical misalignment\n",
    "def process_ver_misalignment_data(suffle_data=True, test_size=0.3):\n",
    "    \n",
    "    # VER_MISLAGNMENT_OUTPUT_FILES=[\"ver_misalignment_low_data.csv\", \"ver_misalignment_medium_data.csv\", \"ver_misalignment_high_data.csv\"]\n",
    "\n",
    "\n",
    "    # if (Path.exists( OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_data.csv\")) ):\n",
    "    #     return\n",
    "\n",
    " \n",
    "\n",
    "    OUTPUT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    # Y values appedend to the list\n",
    "    LOW_VER_MISALIGNMENT_OUTPUT = [3, 1.0]\n",
    "    MEDIUM_VER_MISALIGNMENT_OUTPUT = [3, 2.0]\n",
    "    HIGH_VER_MISALIGNMENT_OUTPUT = [3, 3.0]\n",
    "\n",
    "    total_lines = 0\n",
    "\n",
    "    # Low criticality (6g, 10g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_LOW_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")\n",
    "                for data in data_iter:\n",
    "                    data.extend(LOW_VER_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "            \n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)             \n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_low_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Low data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # Medium criticality (15g, 20g, 25g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_MEDIUM_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(MEDIUM_VER_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)\n",
    "\n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "\n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_medium_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"Medium data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0\n",
    "\n",
    "    # High criticality (30g, 35g)\n",
    "    if not ( Path.exists(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data.csv\")) ):\n",
    "        for file_name in IMBALANCE_HIGH_FILES:\n",
    "\n",
    "            data_list = list()\n",
    "\n",
    "            with open(file_name, 'r') as data_file:\n",
    "                data_iter = csv.reader(data_file, delimiter=\",\")                \n",
    "                for data in data_iter:\n",
    "                    data.extend(HIGH_VER_MISALIGNMENT_OUTPUT)\n",
    "                    data_list.append(data)    \n",
    "\n",
    "            if (suffle_data):\n",
    "                random.suffle(data_list)\n",
    "            \n",
    "            with open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data_treinamento.csv\"), 'a') as training_file, \\\n",
    "                 open(OUTPUT_DATA_DIR.joinpath(\"ver_misalignment_high_data_validacao.csv\"), 'a') as test_file:\n",
    "            \n",
    "                data_size = len(data_list)\n",
    "                test_index = floor(data_size * test_size)\n",
    "                total_lines += data_size \n",
    "            \n",
    "                test_writer = csv.writer(test_file)\n",
    "                training_writer = csv.writer(training_file)\n",
    "\n",
    "                test_writer.writerows(data_list[:test_index])\n",
    "                training_writer.writerows(data_list[test_index:])\n",
    "    \n",
    "    print(\"High data\")\n",
    "    print(\"######################################\")\n",
    "    print(f\"Dataset size: {total_lines} lines\")\n",
    "    print(f\"Test dataset size {floor(total_lines * test_size)} lines\")\n",
    "    print(f\"Training dataset size {total_lines - floor(total_lines * test_size)} lines\\n\")\n",
    "    \n",
    "    total_lines = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal data\n",
    "process_files(root_dir=NORMAL_BASE_DIR, defect_class='normal', single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imbalance data\n",
    "process_files(root_dir=IMBALANCE_BASE_DIR, defect_class='normal', single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal misalignment data\n",
    "process_files(root_dir=HOR_MISALIGNMENT_BASE_DIR, defect_class='normal', single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertical misalignment data\n",
    "process_files(root_dir=VER_MISALIGNMENT_BASE_DIR, defect_class='normal', single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overhang data\n",
    "process_files(root_dir=OVERHANG_MISALIGNMENT_BASE_DIR, defect_class='normal', single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#underhang data\n",
    "process_files(root_dir=UNDERHANG_MISALIGNMENT_BASE_DIR, defect_class='normal', single_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_DATASET_SIZE_TRAINING = 8575000 # The number of lines of the training set of the Normal data (the smaller one)\n",
    "    MAX_DATASET_SIZE_VALIDATION = 3675000 # The number of lines of the validation set of the Normal data (the smaller)\n",
    "\n",
    "    def __init__(self, batch_size=8575, is_validation=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.is_validation = is_validation\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns the number of batches in the sequence\n",
    "        if not self.is_validation:\n",
    "            return int(floor(DataGenerator.MAX_DATASET_SIZE_TRAINING / self.batch_size))\n",
    "        else:\n",
    "            return int(floor(DataGenerator.MAX_DATASET_SIZE_VALIDATION / self.batch_size))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns the next batch of values based on the index\n",
    "        name_modifier = \"\"\n",
    "        data = pd.DataFrame()\n",
    "        \n",
    "\n",
    "        if self.is_validation:\n",
    "            name_modifier = \"_validacao\"\n",
    "\n",
    "        else:\n",
    "            name_modifier = \"_treinamento\"\n",
    "\n",
    "           \n",
    "        normal_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"normal_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        imbalance_low_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"imbalance_low_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size))\n",
    "        imbalance_medium_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"imbalance_medium_data{name_modifier}.csv\"), chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        imbalance_high_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"imbalance_high_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        hor_misalignment_low_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"hor_misalignment_low_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        hor_misalignment_medium_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"hor_misalignment_medium_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        hor_misalignment_high_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"hor_misalignment_high_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        ver_misalignment_low_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"ver_misalignment_low_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        ver_misalignment_medium_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"ver_misalignment_medium_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "        ver_misalignment_high_data = pd.read_csv(OUTPUT_DATA_DIR.joinpath(f\"ver_misalignment_high_data{name_modifier}.csv\"),chunksize=self.batch_size, header=None, skiprows=(index*self.batch_size) )\n",
    "\n",
    "\n",
    "        data = data.append(normal_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(imbalance_low_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(imbalance_medium_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(imbalance_high_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(hor_misalignment_low_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(hor_misalignment_medium_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(hor_misalignment_high_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(ver_misalignment_low_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(ver_misalignment_medium_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "        data = data.append(ver_misalignment_high_data.get_chunk(self.batch_size), ignore_index=True)\n",
    "\n",
    "\n",
    "        data_x = data.iloc[:,0:8].to_numpy()\n",
    "        data_y = data.iloc[:,8:10].to_numpy().astype(int)\n",
    "\n",
    "        return (data_x, data_y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
